{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f46e62",
   "metadata": {},
   "source": [
    "# Kenyan eCitizen Services Dataset - Processing Exploration Notebook\n",
    "\n",
    "- This notebook serves as an exploration of how to process the scraped data from the Kenyan eCitizen services platform. The goal is to understand how to extract relevant information from the raw HTML files that were saved during the scraping phase, and to identify any challenges or nuances in the data structure that we need to account for in our processing pipeline.\n",
    "- This is a continuation of the [scraping exploration notebook](scraping_exploration.ipynb), where we identified the structure of the pages and how to extract the relevant HTML content. In this notebook, we will focus on how to process that HTML content to extract structured data that can be used readily.\n",
    "- All the extracted data is in the `pages/` directory, and all structured data outputs will be saved in the `data/` directory. We will maintain a consistent naming convention for the output files to ensure that they are easily identifiable and organized.\n",
    "\n",
    "**A note on processing URLs**:\n",
    "\n",
    "- Some links provide relative URLs, which we will need to convert to absolute URLs by prefixing them with the base URL of the eCitizen platform. This is important to ensure that we can access the correct pages when we need to navigate through the links for further data extraction.\n",
    "- This can simply be done by checking if the URL starts with a slash (`/`) and then concatenating it with the base URL (e.g., `https://www.ecitizen.go.ke`) to form the complete URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c754ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9858d26a",
   "metadata": {},
   "source": [
    "## Processing `FAQ` Page \n",
    "\n",
    "- Here we process the `FAQ` page, which contains the frequently asked questions and their answers. \n",
    "- To process this page, we will use BeautifulSoup to parse the HTML content and extract the questions and answers.\n",
    "- Each question and answer pair is within a `li` item with an id in the form of `faq_<number>`, the question is within a `button` tag, and the answer is within the neighboring `div`. To extract this information, we will loop through all the `li` items, extract the question form the `button` tag, and the answer from the `div` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47dd66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAQ_PATH = 'pages/faq.html'\n",
    "FAQ_DATA_PATH_CSV = 'data/faq/faq.csv'\n",
    "FAQ_DATA_PATH_JSON = 'data/faq/faq.json'\n",
    "\n",
    "# Load the FAQ page\n",
    "with open(FAQ_PATH) as f:\n",
    "\tfaq_html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(faq_html, 'lxml')\n",
    "\n",
    "faq_items = soup.find_all('li', id=re.compile(r'^faq_'))\n",
    "\n",
    "faqs = []\n",
    "\n",
    "for item in faq_items:\n",
    "\tbtn = item.find('button')\n",
    "\tif not btn:\n",
    "\t\tcontinue\n",
    "\n",
    "\tquestion = btn.get_text(' ', strip=True)\n",
    "\n",
    "\t# Answer is usually the next sibling div\n",
    "\tanswer_div = btn.find_next_sibling('div')\n",
    "\n",
    "\t# If the DOM is slightly different, fall back to\n",
    "\t# searching inside the li for the first div after button\n",
    "\tif not answer_div:\n",
    "\t\tdivs = item.find_all('div')\n",
    "\t\tanswer_div = divs[0] if divs else None\n",
    "\n",
    "\tanswer = ''\n",
    "\tif answer_div:\n",
    "\t\tanswer = answer_div.get_text(' ', strip=True)\n",
    "\n",
    "\t# Optional: skip empty answers\n",
    "\tif question and answer:\n",
    "\t\tfaqs.append(\n",
    "\t\t\t{\n",
    "\t\t\t\t'id': item.get('id'),\n",
    "\t\t\t\t'question': question,\n",
    "\t\t\t\t'answer': answer,\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(faqs)\n",
    "df.to_csv(FAQ_DATA_PATH_CSV, index=False)\n",
    "\n",
    "# Save to JSON\n",
    "with open(FAQ_DATA_PATH_JSON, 'w') as f:\n",
    "\tjson.dump(faqs, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c943b",
   "metadata": {},
   "source": [
    "## Processing `Agencies` Page\n",
    "\n",
    "- Here we process the `Agencies` page, which contains a list of government agencies and metadata about them.\n",
    "- Each agency is represented as a `a` tag with the link to that agency's page as the `href` attribute of that tag.\n",
    "- To extract the logo url for each agency, we get the `src` attribute of the `img` tag within the `a` tag.\n",
    "- To extract the name of the agency, we get the text content of the `h4` tag within the `a` tag, and to get the description of the agency, we get the text content of the `p` tag within the `a` tag, which is a neighboring tag to the `h4` tag.\n",
    "- We will loop through all the `a` tags and extract this information for each agency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d30eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENCIES_PATH = 'pages/agencies_grid.html'\n",
    "AGENCIES_DATA_PATH_CSV = 'data/agencies/agencies.csv'\n",
    "AGENCIES_DATA_PATH_JSON = 'data/agencies/agencies.json'\n",
    "\n",
    "# Load the Agencies page\n",
    "with open(AGENCIES_PATH) as f:\n",
    "\tagencies_html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(agencies_html, 'lxml')\n",
    "\n",
    "agency_links = soup.find_all('a')\n",
    "\n",
    "agencies = []\n",
    "\n",
    "for link in agency_links:\n",
    "\t# Extract the URL for the agency page\n",
    "\turl = link.get('href', '')\n",
    "\n",
    "\t# Extract the logo URL from the img tag within the link\n",
    "\timg_tag = link.find('img')\n",
    "\tlogo_url = img_tag.get('src', '') if img_tag else ''\n",
    "\n",
    "\t# Extract the agency name from the h4 tag\n",
    "\t# within the link\n",
    "\th4_tag = link.find('h4')\n",
    "\tname = (\n",
    "\t\th4_tag.get_text(' ', strip=True) if h4_tag else ''\n",
    "\t)\n",
    "\n",
    "\t# Extract the description from the p tag within the link\n",
    "\tp_tag = link.find('p')\n",
    "\tdescription = (\n",
    "\t\tp_tag.get_text(' ', strip=True) if p_tag else ''\n",
    "\t)\n",
    "\n",
    "\tif name:\n",
    "\t\tagencies.append(\n",
    "\t\t\t{\n",
    "\t\t\t\t'name': name,\n",
    "\t\t\t\t'description': description,\n",
    "\t\t\t\t'logo_url': logo_url,\n",
    "\t\t\t\t'url': url,\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(agencies)\n",
    "df.to_csv(AGENCIES_DATA_PATH_CSV, index=False)\n",
    "\n",
    "# Save to JSON\n",
    "with open(AGENCIES_DATA_PATH_JSON, 'w') as f:\n",
    "\tjson.dump(agencies, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0653b9e",
   "metadata": {},
   "source": [
    "## Processing `Ministries` Pages\n",
    "\n",
    "- Extracting information about the ministries is a bit more involved, we split this into multiple steps to ensure that we can handle the complexity of navigating through the pages and extracting the relevant information.\n",
    "\n",
    "### 1. Initial Navigation\n",
    "\n",
    "- To extract the names of ministries and the corresponding URLS to their source pages on the eCitizen platform, we can get them from the list of `a` tags in the `National` navigation menu, the URL for each ministry's page is in the `href` attribute and the name of the ministry is in the text content of the `a` tag. \n",
    "- This step is straightforward, we just need to loop through all the `a` tags in the `National` navigation menu and extract this information for each ministry.\n",
    "\n",
    "> Note we do not execute this step in the processing exploration notebook, but we will implement this in the processing pipeline.\n",
    "\n",
    "### 2. Ministry Overview\n",
    "\n",
    "- The ministry overview is processed separately from service information for simplicity\n",
    "- The data is quite structured, a `dl` tag contains relevant metadata about the ministry, the `dt` tag with the text content `Total Agencies` indicates the number of agencies under the ministry, and the corresponding `dd` tag contains the actual number. The `dt` tag with the text content `Total Services` indicates the number of services under the ministry, and the corresponding `dd` tag contains the actual number. This pattern of agencies number followed by services number is consistent across all ministry overview pages, so we can rely on this structure to extract the relevant information.\n",
    "- The description of the ministry is in the sole `article` tag on the page, we can get the text content of that tag to extract the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf71960",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINISTRY_OVERVIEW_PATH = (\n",
    "\t'pages/ministry/ministry_overview.html'\n",
    ")\n",
    "MINISTRY_OVERVIEW_DATA_PATH_CSV = (\n",
    "\t'data/ministry/ministry_overview.csv'\n",
    ")\n",
    "MINISTRY_OVERVIEW_DATA_PATH_JSON = (\n",
    "\t'data/ministry/ministry_overview.json'\n",
    ")\n",
    "\n",
    "# Load the Ministry Overview page\n",
    "with open(MINISTRY_OVERVIEW_PATH) as f:\n",
    "\tministry_overview_html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(ministry_overview_html, 'lxml')\n",
    "ministry_overview = []\n",
    "\n",
    "dd_tags = soup.find_all('dd')\n",
    "\n",
    "# Values are in the next sibling divs\n",
    "total_agencies = (\n",
    "\tdd_tags[0].get_text(strip=True) if dd_tags else None\n",
    ")\n",
    "total_services = (\n",
    "\tdd_tags[1].get_text(strip=True)\n",
    "\tif len(dd_tags) > 1\n",
    "\telse None\n",
    ")\n",
    "\n",
    "# Ministry description is in article tag\n",
    "description_tag = soup.find('article')\n",
    "description = (\n",
    "\tdescription_tag.get_text(' ', strip=True)\n",
    "\tif description_tag\n",
    "\telse None\n",
    ")\n",
    "\n",
    "ministry_overview.append(\n",
    "\t{\n",
    "\t\t'ministry': 'the-state-law-office',\n",
    "\t\t'total_agencies': total_agencies,\n",
    "\t\t'total_services': total_services,\n",
    "\t\t'description': description,\n",
    "\t}\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(ministry_overview)\n",
    "df.to_csv(MINISTRY_OVERVIEW_DATA_PATH_CSV, index=False)\n",
    "\n",
    "# Save to JSON\n",
    "with open(MINISTRY_OVERVIEW_DATA_PATH_JSON, 'w') as f:\n",
    "\tjson.dump(ministry_overview, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea829290",
   "metadata": {},
   "source": [
    "### 3. Ministry Agencies\n",
    "\n",
    "- To get the services under each agency within the ministry, we need to navigate to the ministry page with the appropriate query parameters to get the list of agencies under that ministry, and then extract the relevant information for each agency.\n",
    "- Parsing this information is also quite straightforward, each agency is represented as a `a` tag with the link to that agency's page as the `href` attribute of that tag, and the name of the agency is in the text content of the `a` tag. \n",
    "- This allows us to get the page with the relevant list of services under each agency, and we can then extract the relevant information for each service.\n",
    "- For each URL, we can extract the name of the agency as well as the department it belongs to within the ministry, we extract these from the query parameters in the URL.\n",
    "\n",
    "> This step is also not executed in the processing exploration notebook, but we will implement this in the processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b68357",
   "metadata": {},
   "source": [
    "### 4. Ministry Services\n",
    "\n",
    "- For each service under the agency within the ministry, we need only extract the `a` tag with the link to the service page as the `href` attribute of that tag, and the name of the service is in the text content of the `a` tag.\n",
    "- Fro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ecf201",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINISTRY_SERVICES_PATH = (\n",
    "    'pages/ministry/ministry_services.html'\n",
    ")\n",
    "MINISTRY_SERVICES_DATA_PATH_CSV = (\n",
    "    'data/ministry/ministry_services.csv'\n",
    ")\n",
    "MINISTRY_SERVICES_DATA_PATH_JSON = (\n",
    "    'data/ministry/ministry_services.json'\n",
    ")\n",
    "\n",
    "# Load the Ministry Services page\n",
    "with open(MINISTRY_SERVICES_PATH) as f:\n",
    "    ministry_services_html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(ministry_services_html, 'lxml')\n",
    "\n",
    "service_links = soup.find_all('a')\n",
    "services = []\n",
    "\n",
    "for link in service_links:\n",
    "    url = link.get('href', '')\n",
    "    name = link.get_text(' ', strip=True)\n",
    "\n",
    "    if name:\n",
    "        services.append(\n",
    "            {\n",
    "                'ministry': 'the-state-law-office',\n",
    "                'department': 'registers-generals-department',\n",
    "                'agency': 'registrar-of-marriages',\n",
    "                'name': name,\n",
    "                'url': url,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(services)\n",
    "df.to_csv(MINISTRY_SERVICES_DATA_PATH_CSV, index=False)\n",
    "\n",
    "# Save to JSON\n",
    "with open(MINISTRY_SERVICES_DATA_PATH_JSON, 'w') as f:\n",
    "    json.dump(services, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
